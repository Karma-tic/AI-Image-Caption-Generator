{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXQXnFPGQuwH"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import inception_v3\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "import os\n",
        "import pickle\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount your Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- UPDATE THESE PATHS ---\n",
        "\n",
        "# This is the path to the folder containing your 5,000 validation images\n",
        "VAL_IMG_FOLDER = \"/content/drive/My Drive/Evoastra - Team C Project/COCO2017_unzipped/val2017/\"\n",
        "\n",
        "# This is where your new feature file will be saved\n",
        "OUTPUT_FILE_PATH = \"/content/drive/My Drive/Evoastra - Team C Project/val_features.pkl\"\n",
        "\n",
        "# --- END OF PATHS ---\n",
        "\n",
        "print(f\"Loading images from: {VAL_IMG_FOLDER}\")\n",
        "print(f\"Saving features to: {OUTPUT_FILE_PATH}\")"
      ],
      "metadata": {
        "id": "c7UTV6OoRIYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the InceptionV3 model, pre-trained on ImageNet\n",
        "# include_top=False removes the final classification layer\n",
        "image_model = inception_v3.InceptionV3(weights='imagenet', include_top=False)\n",
        "\n",
        "# Create a new model that outputs the last convolutional layer\n",
        "# This is our feature extractor\n",
        "feature_extractor = Model(inputs=image_model.inputs, outputs=image_model.layers[-1].output)\n",
        "\n",
        "print(\"InceptionV3 feature extractor model loaded.\")"
      ],
      "metadata": {
        "id": "MWKKkrJeROEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_image(image_path):\n",
        "    # Load the image, resizing to 299x299 (required by InceptionV3)\n",
        "    img = load_img(image_path, target_size=(299, 299))\n",
        "\n",
        "    # Convert image to a 3D numpy array\n",
        "    img_array = img_to_array(img)\n",
        "\n",
        "    # Add a 4th dimension (batch size)\n",
        "    img_array = tf.expand_dims(img_array, axis=0)\n",
        "\n",
        "    # Preprocess the image for InceptionV3 (scales pixel values)\n",
        "    img_array = inception_v3.preprocess_input(img_array)\n",
        "\n",
        "    return img_array\n",
        "\n",
        "print(\"Preprocessing function defined.\")"
      ],
      "metadata": {
        "id": "nyfJ9sVQRVo9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the completed dictionary to your Google Drive\n",
        "with open(OUTPUT_FILE_PATH, 'wb') as f:\n",
        "    pickle.dump(features_dict, f)\n",
        "\n",
        "print(f\"Successfully saved all features to: {OUTPUT_FILE_PATH}\")"
      ],
      "metadata": {
        "id": "-cNH1-kYfqH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. INSTALL AND IMPORT LIBRARIES ---\n",
        "# We need to install the 'transformers' library from Hugging Face\n",
        "!pip install transformers\n",
        "\n",
        "import requests\n",
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(\"--- Libraries installed and imported. ---\")\n",
        "\n",
        "# --- 2. LOAD THE PRE-TRAINED MODEL ---\n",
        "# This will download the 'BLIP' model from Salesforce/Hugging Face\n",
        "# It's a large model, so this might take 2-3 minutes.\n",
        "print(\"Loading pre-trained model (Salesforce/blip-image-captioning-large)...\")\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
        "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
        "\n",
        "# If you have a GPU, move the model to the GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(\"--- Model loaded successfully! ---\")\n",
        "\n",
        "\n",
        "# --- 3. LOAD THE TEST IMAGE ---\n",
        "# We'll use the surfer image again to see the difference.\n",
        "image_url = 'https://tensorflow.org/images/surf.jpg'\n",
        "raw_image = Image.open(requests.get(image_url, stream=True).raw).convert('RGB')\n",
        "\n",
        "\n",
        "# --- 4. GENERATE THE CAPTION ---\n",
        "print(\"Generating caption...\")\n",
        "\n",
        "# Process the image (resize, normalize, etc.)\n",
        "inputs = processor(raw_image, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# Generate the caption (this tells the model to create text)\n",
        "out = model.generate(**inputs)\n",
        "\n",
        "# Decode the generated text\n",
        "predicted_caption = processor.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "# --- 5. SHOW THE RESULT ---\n",
        "plt.imshow(raw_image)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "print(f\"MODEL PREDICTION: {predicted_caption}\")"
      ],
      "metadata": {
        "id": "t2P-SK-siWHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import io\n",
        "\n",
        "print(\"--- Upload an image from your computer ---\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get the filename of the uploaded file\n",
        "for fn in uploaded.keys():\n",
        "  print(f'User uploaded file \"{fn}\"')\n",
        "\n",
        "  # Load the uploaded image\n",
        "  raw_image = Image.open(io.BytesIO(uploaded[fn])).convert('RGB')\n",
        "\n",
        "  # Generate the caption (using the model we already loaded)\n",
        "  print(\"Generating caption...\")\n",
        "  inputs = processor(raw_image, return_tensors=\"pt\").to(device)\n",
        "  out = model.generate(**inputs)\n",
        "  predicted_caption = processor.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "  # Show the result\n",
        "  plt.imshow(raw_image)\n",
        "  plt.axis('off')\n",
        "  plt.show()\n",
        "\n",
        "  print(f\"MODEL PREDICTION: {predicted_caption}\")"
      ],
      "metadata": {
        "id": "ltGk5YebkcpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import io\n",
        "import torch\n",
        "from PIL import Image  # <-- [FIX] Import the Image library\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- [FIX] We must also re-define these variables ---\n",
        "# The runtime forgot them when it restarted or timed out.\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
        "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "# ---\n",
        "\n",
        "print(\"--- Upload an image from your computer ---\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get the filename of the uploaded file\n",
        "for fn in uploaded.keys():\n",
        "  print(f'User uploaded file \"{fn}\"')\n",
        "\n",
        "  # Load the uploaded image\n",
        "  raw_image = Image.open(io.BytesIO(uploaded[fn])).convert('RGB')\n",
        "\n",
        "  # Generate the caption (using the model we already loaded)\n",
        "  print(\"Generating caption...\")\n",
        "  inputs = processor(raw_image, return_tensors=\"pt\").to(device)\n",
        "  out = model.generate(**inputs)\n",
        "  predicted_caption = processor.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "  # Show the result\n",
        "  plt.imshow(raw_image)\n",
        "  plt.axis('off')\n",
        "  plt.show()\n",
        "\n",
        "  print(f\"MODEL PREDICTION: {predicted_caption}\")"
      ],
      "metadata": {
        "id": "46gFwQ-2kR_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. ALL IMPORTS ---\n",
        "from google.colab import files\n",
        "import io\n",
        "import torch\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
        "\n",
        "# --- 2. DEFINE MODEL AND PROCESSOR ---\n",
        "print(\"Loading pre-trained model...\")\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
        "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(\"Model is ready.\")\n",
        "\n",
        "# --- 3. UPLOAD SCRIPT ---\n",
        "print(\"\\n--- Upload an image from your computer ---\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get the filename of the uploaded file\n",
        "for fn in uploaded.keys():\n",
        "  print(f'User uploaded file \"{fn}\"')\n",
        "\n",
        "  # Load the uploaded image\n",
        "  raw_image = Image.open(io.BytesIO(uploaded[fn])).convert('RGB')\n",
        "\n",
        "  # Generate the caption\n",
        "  print(\"Generating caption...\")\n",
        "  inputs = processor(raw_image, return_tensors=\"pt\").to(device)\n",
        "  out = model.generate(**inputs)\n",
        "  predicted_caption = processor.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "  # Show the result\n",
        "  plt.imshow(raw_image)\n",
        "  plt.axis('off')\n",
        "  plt.show()\n",
        "\n",
        "  print(f\"MODEL PREDICTION: {predicted_caption}\")"
      ],
      "metadata": {
        "id": "mWY9F6-1lC8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import io\n",
        "import torch\n",
        "from PIL import Image  # <-- [FIX] Import the Image library\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- [FIX] We must also re-define these variables ---\n",
        "# The runtime forgot them when it restarted or timed out.\n",
        "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
        "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "# ---\n",
        "\n",
        "print(\"--- Upload an image from your computer ---\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Get the filename of the uploaded file\n",
        "for fn in uploaded.keys():\n",
        "  print(f'User uploaded file \"{fn}\"')\n",
        "\n",
        "  # Load the uploaded image\n",
        "  raw_image = Image.open(io.BytesIO(uploaded[fn])).convert('RGB')\n",
        "\n",
        "  # Generate the caption (using the model we already loaded)\n",
        "  print(\"Generating caption...\")\n",
        "  inputs = processor(raw_image, return_tensors=\"pt\").to(device)\n",
        "  out = model.generate(**inputs)\n",
        "  predicted_caption = processor.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "  # Show the result\n",
        "  plt.imshow(raw_image)\n",
        "  plt.axis('off')\n",
        "  plt.show()\n",
        "\n",
        "  print(f\"MODEL PREDICTION: {predicted_caption}\")"
      ],
      "metadata": {
        "id": "UFslBQJvmRZl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}