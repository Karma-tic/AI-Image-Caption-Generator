AI Image Caption Generator
ğŸš€ Project Status: [Completed]
This project, part of my AI/ML internship with Evoastra, implements a state-of-the-art Vision-Encoder-Decoder model to generate accurate, human-like captions for any given image.

ğŸ¯ Project Goal
The objective of this project was to build a system that can understand the content of an image and generate a descriptive, coherent caption. Instead of training a simple model from scratch (which can be inefficient), I implemented a state-of-the-art, pre-trained Transformer model to achieve high-quality results immediately.

ğŸ¤– Final Model: Salesforce BLIP
This project uses BLIP (Bootstrapping Language-Image Pre-training), a powerful model from Salesforce/Hugging Face. This Vision-Encoder-Decoder model is pre-trained on massive datasets and is highly effective at understanding the complex relationships between images and text.

ğŸ› ï¸ Tech Stack
Python

Hugging Face transformers (for loading and using the model)

PyTorch

Google Colab (for running the model)

Pillow (PIL) (for image processing)

ğŸŒŸ Final Results & Demo
The model can successfully generate detailed captions for new, unseen images.

Example 1: Surfer

Model Prediction: "surfer riding a wave in the ocean on a sunny day"

Example 2: My image 

Model Prediction: a close up of a man with a mustache and a plaid shirt

You can view the final, working code in the project's Colab Notebook.
